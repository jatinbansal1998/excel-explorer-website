# Plan 09: LLM‑Only Analytics via OpenRouter

## Engineer Assignment

**Primary Engineer**: Frontend/AI Integration Engineer
**Dependencies**: Plan 01 (Infrastructure), Plan 02 (Data Processing), Plan 06 (Utilities), Plan 08 (OpenRouter Integration)
**Estimated Time**: 2–3 days

## Overview

Deliver an analytics experience powered exclusively by an LLM through OpenRouter. Users connect their OpenRouter key, pick a model, then ask analytical questions or choose from suggested prompts. We send minimal dataset context (metadata + small, bounded sample) and render structured “insight cards” and actions returned by the model. No deterministic/local analytics are computed.

References:

- API overview and headers (Authorization, optional HTTP-Referer, X-Title): [OpenRouter API Overview](https://openrouter.ai/docs/api-reference/overview)
- Completions (chat): [OpenRouter Completions API](https://openrouter.ai/docs/api-reference/completion)
- Models and pricing schema: [OpenRouter Models](https://openrouter.ai/docs/models)
- Usage/Credits endpoint context: [Usage Accounting](https://openrouter.ai/docs/use-cases/usage-accounting)

## Goals

- 100% LLM‑driven analytics UI (prompt in, insights out)
- Prompt suggestions generated by the LLM based on current dataset context
- Strict typing for prompts, responses, and actions (chart/filter configs)
- Fully client‑side calls to OpenRouter with encrypted key at rest (from Plan 08)

## Non‑Goals

- No local/deterministic analytics (no numeric profiling, correlations, pivots computed locally)
- No server proxy or backend state

## Types (Strict)

```ts
// src/types/llmAnalytics.ts
export type InsightKind =
  | 'kpi'
  | 'distribution'
  | 'correlation'
  | 'anomaly'
  | 'explanation'
  | 'suggestion'

export interface InsightCard {
  id: string
  title: string
  kind: InsightKind
  details: string // markdown or plain text
  confidence?: number // 0..1
  chartConfigJson?: string // optional serialized ChartConfig
  filterConfigJson?: string // optional serialized FilterConfig[] or single filter
}

export interface PromptSuggestion {
  id: string
  category: 'descriptive' | 'diagnostic' | 'predictive' | 'prescriptive' | 'other'
  prompt: string
  rationale?: string
}

export interface LLMAnalyticsResponse {
  insights: InsightCard[]
  followUps?: PromptSuggestion[]
}
```

## Services

```ts
// src/services/llmAnalytics.ts
export class LLMAnalyticsService {
  constructor(private readonly openrouter: OpenRouterService) {}

  async suggestPrompts(apiKey: string, model: string, context: string): Promise<PromptSuggestion[]>

  async analyze(
    apiKey: string,
    model: string,
    prompt: string,
    context: string,
  ): Promise<LLMAnalyticsResponse>
}
```

- Uses `OpenRouterService.chat(...)` (Plan 08) under the hood.
- Prefer JSON‑first responses (system instruction) with fallback to markdown parsing.
- Include optional headers `HTTP-Referer` and `X-Title` for attribution per docs.

## Prompting Strategy

System message (template):

- Role: system
- Content (high‑level):
  - You are a data analyst assistant. Respond ONLY with minified JSON per schema unless asked for free‑form text.
  - Schema: `LLMAnalyticsResponse` with `insights` and optional `followUps` (PromptSuggestion[]).
  - `chartConfigJson` and `filterConfigJson` must be valid JSON strings adhering to our app’s `ChartConfig` and `FilterConfig` types.
  - If unsure, return empty arrays, don’t fabricate.

User message (suggest prompts):

- Include dataset context: file name, sheet name, columns (name + type) and limited sample rows (e.g., ≤100) or a few value summaries.
- Ask: “Propose 8–15 highly relevant prompts categorized as descriptive/diagnostic/predictive/prescriptive. Provide rationale. JSON only.”

User message (analysis):

- Include dataset context (same as above) + the user’s prompt. Ask for: 5–12 insight cards, each with a clear title, optional action payloads (chart/filter), and a few follow‑up suggestions.

Context payload

- Only: metadata + small bounded sample (≤100 rows) to preserve privacy and token limits.

## Hook

```ts
// src/hooks/useLLMAnalytics.ts
export function useLLMAnalytics() {
  // state: suggestions, insights, isLoading, error
  // actions: fetchSuggestions(), runAnalysis(prompt)
  // integrates with useOpenRouter (Plan 08) for model and apiKey
}
```

- Debounce suggestion requests on dataset change.
- Cache last suggestions per session via LocalStorageManager from Plan 06.

## UI Components

- `src/components/analytics/AnalyticsPanel.tsx`
  - Tabs: Suggestions | Prompt
  - Suggestions tab: list of `PromptSuggestion` grouped by category; click inserts into prompt input
  - Prompt tab: textarea, run button, streaming output of `InsightCard` tiles
  - Each insight card shows title, details (markdown), and buttons to “Apply Chart” / “Apply Filters” if payloads exist

- Integration points
  - “Apply Chart”: deserialize `chartConfigJson` → add via `useCharts`
  - “Apply Filters”: deserialize → apply via `useFilters`

## Data Handling & Privacy

- Do not upload entire dataset. Limit to metadata + ≤100 sampled rows (random or head/tail evenly) embedded in context.
- Surface a note: analysis is generated by an LLM using provided sample/metadata.

## Error Handling

- Use `ErrorHandler` (Plan 06) with `ErrorType.CHART_ERROR` or generic errors for LLM failures/timeouts.
- Graceful fallbacks if JSON parsing fails: attempt markdown parse → show plain text insight as a single card.

## Performance & UX

- Streaming: enable `stream: true` once we add a streaming renderer (phase 2). For MVP, standard request.
- Loading states and retry.
- Debounce suggestions and cancel in‑flight on tab switch.

## Feature Flags

- `llmAnalytics.enabled` (default true)

## Files to Create

- `src/types/llmAnalytics.ts`
- `src/services/llmAnalytics.ts`
- `src/hooks/useLLMAnalytics.ts`
- `src/components/analytics/AnalyticsPanel.tsx`

## Example Minimal Messages

Suggest prompts request (conceptual):

```json
{
  "model": "<selected-model>",
  "messages": [
    { "role": "system", "content": "<system-template>" },
    {
      "role": "user",
      "content": "Here is the dataset context: <metadata+sample>. Propose 10 prompts as JSON."
    }
  ]
}
```

Analysis request (conceptual):

```json
{
  "model": "<selected-model>",
  "messages": [
    { "role": "system", "content": "<system-template>" },
    {
      "role": "user",
      "content": "Dataset context: <metadata+sample>\nUser prompt: <prompt>. Return JSON LLMAnalyticsResponse."
    }
  ]
}
```

## Validation Criteria

- After connecting OpenRouter and selecting a model, Suggestions tab populates relevant prompts.
- Running an analysis produces well‑formed `InsightCard` entries.
- “Apply Chart/Filters” actions work with existing hooks.
- No local analytics performed; only LLM responses drive insights.

## Open Questions

- Default prompt templates to seed the LLM? (We can ship a small curated set as a fallback.)
- JSON‑only vs markdown‑fallback default behavior? (MVP: try JSON, fallback to text card.)

---

This plan adds an LLM‑only analytics surface using OpenRouter chat completions fully client‑side, leveraging strict types and encrypted API keys. Optional headers `HTTP-Referer` and `X-Title` are supported per OpenRouter guidance for client apps.
